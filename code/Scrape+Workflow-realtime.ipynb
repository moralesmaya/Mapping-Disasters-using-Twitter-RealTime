{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Realtime twitter data scraping using Tweepy and SNSscrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "#There are packages that might require installation\n",
    "# !pip install twitterscraper\n",
    "# !pip install tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load packages from tweepy\n",
    "import tweepy as tp\n",
    "\n",
    "#Load packaes from twitterscraper\n",
    "import twitterscraper\n",
    "from twitterscraper import query_tweets\n",
    "\n",
    "#Load package from snscrape to scrape twitters frontend\n",
    "import snscrape.modules.twitter as sntwitter\n",
    "\n",
    "#NLP Modules\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from keras import utils\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import load_model\n",
    "\n",
    "import gmplot\n",
    "\n",
    "#Load additional packages\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import re\n",
    "import itertools\n",
    "from collections import Counter\n",
    "import sys\n",
    "from retry import retry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insert your twitter api keys below as strings in order for the rest of the book to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Personal APIs (NEED TO CONVERT THIS TO LOCAL VARIABLES BEFORE HANDING IN)\n",
    "#Add API keys here\n",
    "consumer_key = 'Be42F1ILEuSM8t9UNNwywpNtw'\n",
    "consumer_secret = '0B8dI0bwTBPIpWGU0lT7LamwVHK2mIC2k0E1ErXsHCvEKERnxG'\n",
    "bearer_token = 'AAAAAAAAAAAAAAAAAAAAAEDjMgEAAAAAo%2BG1VrFDLs5HxCECRlfgvNqg40Q%3Dr0MdX6JhXhimozIjUF6DiWkAFNEeSa1Vn263J5yu2mF2ZkDOnj'\n",
    "access_token = '983900877919588352-ah6LqQhYEh5QjRBZ9TiBaSsFceOvaRG'\n",
    "access_token_secret = 'kcLhKIaRK8FL4Dh6Tmw9xFiLaTpLj9Zesxh53mrABSwS5'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Authorize api keys in order to use API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Authorization using api keys\n",
    "auth = tp.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tp.API(auth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the top 100 twitter accounts posting natural disaster related news and append disasterchart and Distaster_Update to the list to make a total of 102 accounts to scrape for status updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the top 100 twitter natural disaster reporting accounts\n",
    "df_100 = pd.read_csv('data/top100_twittwer_disaster_accounts.csv')\n",
    "df_100['Account Handle'] = df_100['Account Handle'].str.replace('\\W', '')\n",
    "#Convert account handles to series\n",
    "s_100 = df_100['Account Handle']\n",
    "s_add = pd.Series(['disasterschart','Disaster_Update'])\n",
    "s_102 = s_100.append(s_add)\n",
    "\n",
    "##Initialize a list\n",
    "online = []\n",
    "\n",
    "#Loop to check whether account names are still active or not\n",
    "for screen_name in s_102:\n",
    "    try:\n",
    "        user = api.get_user(screen_name)\n",
    "        online.append(user.screen_name)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "#Final series of active account names\n",
    "s = pd.Series(online)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions to scrape twitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tweepy scrape function is used to scrape the top 102 account statuses for updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initilization\n",
    "array = [[]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pass in any series list of twitter account names\n",
    "@retry(tries=10,delay=2,backoff=4,max_delay=42)\n",
    "def get_top_tweets(s,num_posts):\n",
    "    for screen_name in s:\n",
    "        try:\n",
    "            #Initialize\n",
    "            hashtag_list = []\n",
    "            data = []\n",
    "            #Define user\n",
    "            user = api.get_user(screen_name)\n",
    "            #Only pull the last __ number of tweets from each account \n",
    "            for tweet in api.user_timeline(screen_name = screen_name, count = num_posts):\n",
    "                data.append(f'{screen_name}')\n",
    "                data.append(tweet.created_at) \n",
    "                data.append(tweet.text)\n",
    "                if len(tweet.entities.get('hashtags'))>0:\n",
    "                    ht = [tweet.entities.get('hashtags')[x]['text'] for x in range(0,len(tweet.entities.get('hashtags')))]\n",
    "                    hashtag_list.append(ht)\n",
    "                data.append(hashtag_list)\n",
    "                array.append(data)\n",
    "                data = []\n",
    "                hashtag_list = []\n",
    "\n",
    "        except tp.TweepError:\n",
    "            time.sleep(60*15)\n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a fucntion to convert list items to a single string joined by delimeter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a function to convert list items to a single string joined by semicolon \n",
    "def list_to_string(x):\n",
    "    lis = x\n",
    "    string = ''\n",
    "    string = \";\".join(lis)\n",
    "    return string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a funtion to search the dataframe containing tweets and pull out the unique hashtags and count their mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hts_extract(df):\n",
    "    #Convert hashtags columns to a list\n",
    "    hashtags = df['hashtags'].tolist()\n",
    "\n",
    "    #Initialize two empty lists for temp storage\n",
    "    hts = []\n",
    "    ht = []\n",
    "    ht_final = []\n",
    "\n",
    "    #Iterate through and remove blank hashtags\n",
    "    for x in hashtags:\n",
    "        if x == '':\n",
    "            pass\n",
    "        else:\n",
    "            hts.append(x)\n",
    "\n",
    "    #Iterate through and split out multiple hastags into additional list elements\n",
    "    for x in hts:\n",
    "        if ';' in x:\n",
    "            el = x.split(';')\n",
    "            ht = ht + el\n",
    "        else:\n",
    "            ht.append(x)\n",
    "\n",
    "    for item in ht:\n",
    "        ht_final.append(item.lower())\n",
    "    \n",
    "    #Count the number of times a hastag was mentioned, store this in a dictionary along with the keyword \n",
    "    #Sort the dictionary from highest to lowest values\n",
    "    top_hts = dict(sorted(Counter(ht_final).items(), key=lambda item: item[1],reverse=True))\n",
    "    return top_hts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to get relevant tweets with adjustable parameters of number of queries, start date and end date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "@retry(tries=10,delay=2,backoff=4,max_delay=42)\n",
    "def get_relevant_tweets(num_queries, start_date, end_date):\n",
    "    \n",
    "    #Intitilize empty list \n",
    "    tweets_list = [] \n",
    "\n",
    "    #For loop to go through each of the relevant keyword \n",
    "    for word in set_key_words:\n",
    "\n",
    "        for i,tweet in enumerate(sntwitter.TwitterSearchScraper(f'{word} since:{start_date} until:{end_date}').get_items()):\n",
    "            if i>num_queries:\n",
    "                break\n",
    "            tweets_list.append([tweet.username,tweet.date,tweet.content])\n",
    "    \n",
    "    return tweets_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to pre-process tweet text, toeknize, lematize and remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tweet(df):\n",
    "    tokenizer = RegexpTokenizer('\\w+|\\$[\\.]\\S+')\n",
    "    lem = WordNetLemmatizer()\n",
    "    STOPWORDS = set(stopwords.words('english'))\n",
    "    \n",
    "    df.rename(columns={'text':'tweet'},inplace=True)\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    df.reset_index(inplace=True)\n",
    "    df['processed_tweets'] = df['tweet'].apply(tokenizer.tokenize)\n",
    "    df['processed_tweets'] = df['processed_tweets'].apply(lambda row: list([lem.lemmatize(i) for i in row]))\n",
    "    df['processed_tweets'] = df['processed_tweets'].apply(lambda x:[i for i in x if i not in STOPWORDS] )\n",
    "    \n",
    "    tokenizer = Tokenizer(num_words= 5000)\n",
    "    tokenizer.fit_on_texts(df.processed_tweets.tolist())\n",
    "    X = tokenizer.texts_to_sequences(df.processed_tweets.tolist())\n",
    "    X = pad_sequences(X, maxlen = df.processed_tweets.str.len().max(), padding = 'post')\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define varaible for list of Keywords related to our target label tags coming out of the word2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Keywords related to our target label tags coming out of the word2vec model\n",
    "\n",
    "key_words = ['flooding', 'mudslides', 'droughts', 'landslides', 'flood', 'storms', 'wildfires',\n",
    "             'earthquakes', 'tornadoes','floodwaters', 'typhoons', 'disasters','tsunami', 'tsunamis', \n",
    "             'earthquake', 'explosions', 'exploded', 'explodes', 'blast', 'exploding', 'explode', 'bomb', \n",
    "             'detonation', 'implosion', 'earthquake', 'detonations', 'catastrophe', 'burst', 'fires', 'blasts',\n",
    "             'quake', 'tsunami', 'earthquakes', 'aftershocks', 'quakes','floods', 'temblor', 'disaster', \n",
    "             'devastation', 'tsunamis', 'landslides', 'disasters', 'magnitude', 'mudslides', 'flood',\n",
    "             'hurrican', 'storm', 'hurricanes', 'katrina', 'storms', 'tornado', 'cyclone', 'tsunami', 'typhoon',\n",
    "             'disaster', 'landfall', 'earthquake', 'flood', 'huricane', 'tornadoes', 'tornadoes', 'tornados',\n",
    "             'storm', 'thunderstorm', 'hurricane', 'tsunami', 'cyclone', 'storms', 'typhoon', 'floods',\n",
    "             'snowstorm', 'tornadic', 'flood', 'earthquake', 'waterspout', 'bombings', 'bombed', 'bomb',\n",
    "             'airstrikes', 'bomber', 'bombers', 'bombs', 'terrorist', 'airstrike', 'firebombing', 'attack', \n",
    "             'hijackings', 'attacks', 'bombardment', 'bombardments', 'mudslides', 'landslides', 'landslide', \n",
    "             'rainstorms', 'sinkhole', 'mudflows', 'earthquake', 'rockslide', 'snowstorm', 'kahlua', 'temblor',\n",
    "             'mudflow', 'rainstorm', 'blizzards', 'snowstorms', 'snowpocalypse', 'snowstorm', 'snowstorms', \n",
    "             'snowmageddon', 'snowicane', 'rainstorm', 'squalls', 'snowtober', 'thundersnow', 'whiteout',\n",
    "             'snowfall', 'pogies', 'windchills', 'blizzards', 'bostonist']\n",
    "    \n",
    "#Isolate unique keywords\n",
    "set_key_words = set(key_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_preds(df):\n",
    "    df['pred_label'] = df['predictions'].map({0: \"bombing\", 1: \"earthquake\", 2 : \"explosion\", 3: \"floods\" ,\n",
    "                                              4: \"hurricane\" ,5: \"mudslide\" ,6: \"neutral\" ,7:\"noreaster\" ,\n",
    "                                              8:\"tornado\", 9: \"wildfire\"})\n",
    "    \n",
    "def threshold_counts(df, threshold):\n",
    "    counts = df.pred_label.value_counts()\n",
    "    labels = df.pred_label.unique()\n",
    "    disaster_counts = list(zip( labels, counts))\n",
    "    flagged_disaster = []\n",
    "    [flagged_disaster.append([f'There are {value} mentions of {key}']) for key, value in disaster_counts if value >= threshold]\n",
    "    return flagged_disaster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities = pd.read_csv('https://raw.git.generalassemb.ly/noahszuckerman/project-5-natty-ds/master/data/worldcities.csv?token=AAAH7Z45CMEZ3KD2BQCU23LAGU4UQ', encoding = 'latin')\n",
    "cities.drop(columns = ['city_ascii', 'iso2', 'iso3', 'admin_name', 'capital',\n",
    "       'population', 'id'], axis = 1, inplace = True)\n",
    "\n",
    "# Joining the coordinates to create a tuple.\n",
    "select_coords = list(zip(cities['lat'], cities['lng']))\n",
    "\n",
    "#Filtering out cities in Canada and the United States for demonstration purposes.\n",
    "#Additional cities will be included in production. \n",
    "select_cities = cities.loc[(cities['country'] == 'United States') | (cities['country'] == 'Canada')]\n",
    "select_cities = select_cities['city'].tolist()\n",
    "select_cities = [x.lower() for x in select_cities]\n",
    "city_dict = dict(zip(select_cities,select_coords))\n",
    "\n",
    "\n",
    "def map_location(df):\n",
    "    location = []\n",
    "    array = [[]]\n",
    "\n",
    "    for tweet in df['processed_tweets']:\n",
    "\n",
    "        for word in tweet:\n",
    "            try:\n",
    "                city_dict[word]\n",
    "                location.append(tweet)\n",
    "                location.append(word)\n",
    "                array.append(location)\n",
    "                location = []\n",
    "\n",
    "\n",
    "            except:\n",
    "                location.append(tweet)\n",
    "                location.append('No Location Available')\n",
    "                array.append(location)\n",
    "                location = []\n",
    "\n",
    "    location_df = pd.DataFrame(array, columns = ['tweet', 'location'])\n",
    "    location_df.drop([0], inplace = True)\n",
    "\n",
    "    location_df['tweet'] =  location_df['tweet'].apply(' '.join)\n",
    "    location_df.drop_duplicates(inplace = True)\n",
    "    return location_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('Word2Vec_LSTM_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mathea/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \r",
      "| Time: 19:37:12 | {'covid19': 48, 'earthquake': 17, 'resilient': 10, 'cities': 10, 'iati': 6} | MODEL PREDICTIONS: | [['There are 675 mentions of hurricane'], ['There are 291 mentions of mudslide'], ['There are 226 mentions of earthquake'], ['There are 201 mentions of neutral'], ['There are 189 mentions of tornado'], ['There are 95 mentions of bombing'], ['There are 64 mentions of noreaster'], ['There are 64 mentions of floods'], ['There are 29 mentions of wildfire'], ['There are 28 mentions of explosion']],                                                    pred_label  location             \n",
      "bombing     No Location Available    29 \n",
      "            friendly                 1  \n",
      "            pace                     1  \n",
      "            page                     1  \n",
      "earthquake  No Location Available    201\n",
      "                                    ... \n",
      "wildfire    middle                   1  \n",
      "            mission                  1  \n",
      "            ontario                  1  \n",
      "            union                    1  \n",
      "            wall                     1  \n",
      "Length: 115, dtype: int64 "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-212-c8f2948623bd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhr\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m24\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "x = 1\n",
    "\n",
    "while x> 0:\n",
    "    \n",
    "    #Scrape the top accounts last 10 posts\n",
    "    get_top_tweets(s,10) #10 can be adjusted up or down. Any duplciates will be removed in the following transformations\n",
    "\n",
    "    #Create a dataframe from array\n",
    "    df = pd.DataFrame(array,columns=['screen_name','created_at','tweet','hashtags'])\n",
    "    df.dropna(inplace = True)\n",
    "\n",
    "    #Flatten out the hastags column for extraction\n",
    "    df['hashtags'] = [list(itertools.chain.from_iterable(x)) for x in df['hashtags']]\n",
    "    df['hashtags'] = df['hashtags'].apply(list_to_string)\n",
    "\n",
    "    #Remove any dduplicate values in case accounts have not posted anything new\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    df.reset_index(drop=True,inplace=True)\n",
    "    \n",
    "    #Call function to build sorted dictionary of top hashtag mentions\n",
    "    top_hashtags_dic = hts_extract(df)\n",
    "    \n",
    "    top_hashtags_dic_filt = {k: v for k, v in top_hashtags_dic.items() if v > 5}\n",
    "    \n",
    "    for hr in range(0,25):\n",
    "        \n",
    "        #Reset tweet list to empty\n",
    "        tweets_list = []\n",
    "        \n",
    "        #Call the get relevant tweets function to query all twitter posts for content matching our word2vec keywords\n",
    "        tweets_list = get_relevant_tweets(num_queries=12, start_date = '2021-2-14', end_date = '2021-2-15')\n",
    "        \n",
    "        #Put the tweets list pull into a dataframe\n",
    "        df2 = pd.DataFrame(tweets_list, columns=['screen_name','created_at','tweet'])\n",
    "\n",
    "        #Remove the 'hashtags column' in the first dataset of tweets\n",
    "        df1 = df[['screen_name','created_at','tweet']]\n",
    "\n",
    "        #Combine both datasets\n",
    "        df_ex = df1.append(df2)\n",
    "        \n",
    "        #Remove any dduplicate values in case accounts have not posted anything new\n",
    "        df_ex.drop_duplicates(inplace=True)\n",
    "        df_ex.reset_index(drop=True,inplace=True)\n",
    "        \n",
    "        #Pre-prcoess tweet text for model predictions\n",
    "        \n",
    "        df_ex = df_ex.assign(id=(df_ex['tweet'] + '_' + df_ex['screen_name']).astype('category').cat.codes)\n",
    "        df_ex.id = df_ex.id.astype(str)\n",
    "        df_ex['tweet'] = df_ex['tweet'] + ' 00' + df_ex['id']\n",
    "\n",
    "        X = process_tweet(df_ex)\n",
    "\n",
    "        df_ex['predictions'] = model.predict_classes(X)\n",
    "        map_preds(df_ex)\n",
    "        flagged_mentions = threshold_counts(df_ex, 10)  \n",
    "        trial = map_location(df_ex)\n",
    "\n",
    "\n",
    "        trial['id'] = [i[-4:] for i in trial['tweet']]\n",
    "\n",
    "        df_ex['id'] = ' 00' + df_ex['id']\n",
    "        df_ex['id'] = [i[-4:] for i in df_ex['id']]\n",
    "\n",
    "        final_df = pd.merge(df_ex, trial, how = 'left', on = 'id')\n",
    "\n",
    "        location_groups = (final_df.groupby(['pred_label', 'location']).size())\n",
    "\n",
    "\n",
    "        \n",
    "        #Display results\n",
    "        t = time.localtime()\n",
    "        current_time = time.strftime(\"%H:%M:%S\",t) \n",
    "        sys.stdout.write(f' \\r| Time: {current_time} | {top_hashtags_dic_filt} | MODEL PREDICTIONS: | {flagged_mentions},                                                    {location_groups} ')\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "        time.sleep(60*60)\n",
    "        \n",
    "        if hr == 24:\n",
    "            break\n",
    "    \n",
    "    \n",
    "    t_now = time.asctime( time.localtime(time.time()))\n",
    "    \n",
    "    #Export the dataset as csv\n",
    "    final_df.to_csv(f'data/predictions_{t_now}.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
